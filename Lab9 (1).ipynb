{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gob_PP53VKHD",
        "outputId": "07c9317e-7c71-4816-8539-1689c9277ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5612: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.2616 - accuracy: 0.9241 - val_loss: 0.1300 - val_accuracy: 0.9629\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.1107 - accuracy: 0.9669 - val_loss: 0.1004 - val_accuracy: 0.9715\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0716 - accuracy: 0.9786 - val_loss: 0.0831 - val_accuracy: 0.9751\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0522 - accuracy: 0.9844 - val_loss: 0.0803 - val_accuracy: 0.9770\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0380 - accuracy: 0.9882 - val_loss: 0.0854 - val_accuracy: 0.9756\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0304 - accuracy: 0.9908 - val_loss: 0.0872 - val_accuracy: 0.9764\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0222 - accuracy: 0.9932 - val_loss: 0.0818 - val_accuracy: 0.9793\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0184 - accuracy: 0.9941 - val_loss: 0.0773 - val_accuracy: 0.9806\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.0900 - val_accuracy: 0.9759\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.0798 - val_accuracy: 0.9804\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0798 - accuracy: 0.9804\n",
            "Validation accuracy: 98.04%\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 17.8805 - accuracy: 0.9778\n",
            "Test accuracy: 97.78%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the training data\n",
        "train = pd.read_csv(\"/content/mnist_train.csv\")\n",
        "train_images = train.iloc[:, 1:] / 255\n",
        "train_labels = train.label\n",
        "\n",
        "# Convert the image data to tensor\n",
        "train_images = train_images.values.reshape(train_images.shape[0], 28, 28)\n",
        "train_images = tf.convert_to_tensor(train_images)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_images, val_images = train_images[:50000], train_images[50000:]\n",
        "train_labels, val_labels = train_labels[:50000], train_labels[50000:]\n",
        "\n",
        "# Load the test data\n",
        "test = pd.read_csv(\"/content/mnist_test.csv\")\n",
        "test_images = test.drop(columns='label')\n",
        "test_labels = test.label\n",
        "\n",
        "# Convert the test image data to tensor\n",
        "test_images = test_images.values.reshape(test_images.shape[0], 28, 28)\n",
        "test_images = tf.convert_to_tensor(test_images)\n",
        "\n",
        "# Multi-Layer Perceptron (ANN) Model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(28, 28)))\n",
        "model.add(Dense(units=200, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=10,\n",
        "    validation_data=(val_images, val_labels)\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(val_images, val_labels)\n",
        "print('Validation accuracy: {}%'.format(round(val_acc * 100, 2)))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('Test accuracy: {}%'.format(round(test_acc * 100, 2)))\n"
      ]
    }
  ]
}